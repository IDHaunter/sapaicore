apiVersion: argoproj.io/v1alpha1
kind: WorkflowTemplate
metadata:
  name: train-entities-pipeline-gpu-train-l # executable id, must be unique across all your workflows (YAML files)
  annotations:
    scenarios.ai.sap.com/description: "Train entities scenario: GPU (Train-L plan) + Amazon S3"
    scenarios.ai.sap.com/name: "Entities: GPU Train-L" # Scenario name should be the use case
    executables.ai.sap.com/description: "Train with data from Amazon S3 using GPU (Train-L plan)"
    executables.ai.sap.com/name: "Execute training entities using GPU Train-L" # Executable name should describe the workflow in the use case
    artifacts.ai.sap.com/entitiesdataset.kind: "dataset" # Helps in suggesting the kind of inputs that can be attached.
    artifacts.ai.sap.com/entitiesmodel.kind: "model" # Helps in suggesting the kind of artifact that can be generated.
  labels:
    scenarios.ai.sap.com/id: "train-entities-gpu-train-l"
    ai.sap.com/version: "1.0.2"
spec:
  imagePullSecrets:
    - name: skybufferrepo # docker registry secret
  entrypoint: trainpipeline
  arguments:
    parameters: # placeholder for string like inputs
        - name: MODEL_GUID # identifier local to this workflow
        - name: LEARNING_RATE # identifier local to this workflow
        - name: MAX_EPOCHS # identifier local to this workflow
        - name: MINI_BATCH_SIZE # identifier local to this workflow
  templates:
  - name: trainpipeline
    steps:
    - - name: mypredictor
        template: mycodeblock1
  - name: mycodeblock1
    metadata:
      labels:
          ai.sap.com/resourcePlan: train.l
    inputs:
      artifacts:  # placeholder for cloud storage attachements
        - name: entitiesdataset # a name for the placeholder
          path: /app/data/ # where to copy dataset in the Docker image
    outputs:
      artifacts:
        - name: entitiesmodellocal # local identifier name to the workflow
          globalName: entitiesmodel # name of the artifact generated, and folder name when placed in S3, complete directory will be `../<executaion_id>/entitiesmodel`. Also used above in annotation
          path: /app/model/ # from which folder in docker image (after running workflow step) copy contents to cloud storage
          archive:
            none:   # specify not to compress while uploading to cloud
              {}
    container:
      image: docker.io/skybuffer/sap-ai-core:train-entities-gpu # Docker image name
      imagePullPolicy: Always
      command: ["/bin/sh", "-c"]
      env:
        - name: MODEL_GUID # name of the environment variable inside Docker container
          value: "{{workflow.parameters.MODEL_GUID}}" # value to set from local (to workflow) variable MODEL_GUID
        - name: LEARNING_RATE # name of the environment variable inside Docker container
          value: "{{workflow.parameters.LEARNING_RATE}}" # value to set from local (to workflow) variable LEARNING_RATE
        - name: MAX_EPOCHS # name of the environment variable inside Docker container
          value: "{{workflow.parameters.MAX_EPOCHS}}" # value to set from local (to workflow) variable MAX_EPOCHS
        - name: MINI_BATCH_SIZE # name of the environment variable inside Docker container
          value: "{{workflow.parameters.MINI_BATCH_SIZE}}" # value to set from local (to workflow) variable MINI_BATCH_SIZE
      args:
        - "python3.9 /app/src/main.py"